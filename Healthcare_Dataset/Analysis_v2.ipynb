{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd915b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c18f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import torch\n",
    "import pdb\n",
    "import random\n",
    "import tables\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, average_precision_score,\\\n",
    "balanced_accuracy_score\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "from dill.source import getsource\n",
    "from dill import detect\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b407c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca226e47",
   "metadata": {},
   "source": [
    "### set the seeds and change to current directory + set the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "np.random.seed(SEED)\n",
    "os.environ['USER_PATH']='/share/pierson/selective_labels_data/hirid_data_analysis/richras_dir/learning_from_doctor_and_patient/'\n",
    "os.environ['OUT_PATH']='/share/pierson/selective_labels_data/hirid_data_analysis/richras_dir/learning_from_doctor_and_patient/output_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/share/pierson/selective_labels_data/hirid_data_analysis/richras_dir/learning_from_doctor_and_patient/')\n",
    "from AnalysisFuncs import plotCorr, getPred_fromProb, getMetrics, getCorrMedical, getGroundTruth, getURange\n",
    "from AnalysisFuncs import getResiduals, plotDistributionProbs, plotCorr_w_Unobs\n",
    "from AnalysisFuncs import saveFile, loadFile, getTrainTestIdx, getLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75551144",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path=osp.join(os.environ.get('USER_PATH'), 'HIRID_Repo', 'logs', 'benchmark_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_rep.pkl')\n",
    "test_y_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_label.pkl')\n",
    "test_ids_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_patient_ids.pkl')\n",
    "\n",
    "test_X_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_rep.pkl')\n",
    "test_y_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_label.pkl')\n",
    "test_ids_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_patient_ids.pkl')\n",
    "\n",
    "test_X_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_rep.pkl')\n",
    "test_y_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_label.pkl')\n",
    "test_ids_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_patient_ids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_rep.pkl')\n",
    "test_y_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_label.pkl')\n",
    "test_ids_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111'), 'test_patient_ids.pkl')\n",
    "\n",
    "test_X_D_given_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_rep.pkl')\n",
    "test_y_D_given_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_label.pkl')\n",
    "test_ids_D_given_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111'), 'test_patient_ids.pkl')\n",
    "\n",
    "test_X_D_and_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_rep.pkl')\n",
    "test_y_D_and_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_label.pkl')\n",
    "test_ids_D_and_T_wo_cutoff = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111'), 'test_patient_ids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured'), 'val_rep.pkl')\n",
    "val_y_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "                '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured'), 'val_labels.pkl')\n",
    "val_ids_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured'), 'val_pids.pkl')\n",
    "\n",
    "val_X_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold'), 'val_rep.pkl')\n",
    "val_y_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold'), 'val_labels.pkl')\n",
    "val_ids_D_given_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold'), 'val_pids.pkl')\n",
    "\n",
    "val_X_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease'), 'val_rep.pkl')\n",
    "val_y_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease'), 'val_labels.pkl')\n",
    "val_ids_D_and_T = loadFile(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "        '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease'), 'val_pids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec37b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trained models\n",
    "LGBM_w_feat_v2_cutoff_T = joblib.load(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured', '1111', 'model.pkl'))\n",
    "LGBM_w_feat_v2_cutoff_D_given_T = joblib.load(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Above_Threshold', '1111', 'model.pkl'))\n",
    "LGBM_w_feat_v2_cutoff_D_and_T = joblib.load(osp.join(processed_data_path,'LGBM_w_feat_v2_cutoff_T', \n",
    "    '_depth_7_subsample-data_1.0_subsample-feat_1.0', 'Lactate_Measured_And_Disease', '1111', 'model.pkl'))\n",
    "LGBM_D_given_T_ipw = loadFile(osp.join(processed_data_path, 'LGBM_w_feat_v2_cutoff_T', 'predict_D_given_T_ipw'),\n",
    "         'LGBM_D_given_T_ipw.pkl')\n",
    "LGBM_D_pseudo = loadFile(osp.join(processed_data_path, 'LGBM_w_feat_v2_cutoff_T','predict_D_pseudo'), \n",
    "         'LGBM_D_pseudo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['LGBM']\n",
    "tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T','D_pseudo']\n",
    "clf_list=[LGBM_w_feat_v2_cutoff_T, LGBM_w_feat_v2_cutoff_D_given_T, LGBM_D_given_T_ipw,\n",
    "          LGBM_w_feat_v2_cutoff_D_and_T, LGBM_D_pseudo]\n",
    "clf_dict={}\n",
    "i=0\n",
    "for t in tasks:\n",
    "    clf_dict[t]={}\n",
    "    for m in models:\n",
    "        clf_dict[t][m]=clf_list[i]\n",
    "        i+=1\n",
    "dict_df_labels={}\n",
    "dict_df_probs={}\n",
    "dict_df_ids={}\n",
    "dict_models={}\n",
    "probs_path=processed_data_path\n",
    "alpha=0.1\n",
    "figsize1=(5,50)\n",
    "figsize2=(10,10)\n",
    "df_pp=pd.DataFrame({'AUC' : [],'PR':[],'BalancedAcc':[],'modelName':[],'rowName':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe of labels by matching it with the patient ids\n",
    "df_labels = pd.DataFrame([])\n",
    "df_labels['pids'] = np.concatenate((test_ids_T, val_ids_T))\n",
    "df_labels['T'] = np.concatenate((test_y_T, val_y_T))\n",
    "test_y_D_given_T_all = getLabels(test_y_D_given_T, test_ids_D_given_T, test_ids_T)\n",
    "val_y_D_given_T_all = getLabels(val_y_D_given_T, val_ids_D_given_T, val_ids_T)\n",
    "\n",
    "test_y_D_and_T_all = getLabels(test_y_D_and_T, test_ids_D_and_T, test_ids_T)\n",
    "val_y_D_and_T_all = getLabels(val_y_D_and_T, val_ids_D_and_T, val_ids_T)\n",
    "\n",
    "df_labels['D'] = np.concatenate((test_y_D_given_T_all, val_y_D_given_T_all))\n",
    "df_labels['D_and_T'] = np.concatenate((test_y_D_and_T_all, val_y_D_and_T_all))\n",
    "\n",
    "test_idxs = df_labels.index[df_labels['pids'].isin(test_ids_T)].values\n",
    "cal_idxs = df_labels.index[df_labels['pids'].isin(val_ids_T)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c220c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7623d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_wo_cutoff = pd.DataFrame([])\n",
    "df_labels_wo_cutoff['pids'] = test_ids_T_wo_cutoff\n",
    "df_labels_wo_cutoff['T'] = test_y_T_wo_cutoff\n",
    "test_y_D_given_T_wo_cutoff_all = getLabels(test_y_D_given_T_wo_cutoff, test_ids_D_given_T_wo_cutoff,\n",
    "                                           test_ids_T_wo_cutoff)\n",
    "\n",
    "\n",
    "test_y_D_and_T_wo_cutoff_all = getLabels(test_y_D_and_T_wo_cutoff, test_ids_D_and_T_wo_cutoff,\n",
    "                                         test_ids_T_wo_cutoff)\n",
    "\n",
    "df_labels_wo_cutoff['D|T'] = test_y_D_given_T_wo_cutoff_all\n",
    "df_labels_wo_cutoff['D_and_T'] = test_y_D_and_T_wo_cutoff_all\n",
    "\n",
    "test_idxs_wo_cutoff = df_labels_wo_cutoff.index[df_labels_wo_cutoff['pids'].isin(test_ids_T_wo_cutoff)].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_wo_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de375d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94464e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate_method = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_labels, dict_df_probs, dict_models, df_pp = getCorrMedical(models, tasks, test_X.copy(), clf_dict,\n",
    "        df_labels.copy(), \n",
    "        test_idxs.copy(), dict_df_labels, dict_df_probs, dict_models, df_pp, probs_path, calibrate=True, \n",
    "        figsize1=figsize1, figsize2=figsize2, alpha=alpha, cal_idxs=cal_idxs, cal_X=val_X,\n",
    "                                                                   calibrate_method=calibrate_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat=dict_df_probs['LGBM'].corr(method='pearson')\n",
    "np.linalg.eigvals(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12235bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='medical correlation matrix'\n",
    "corr_method=\"spearman\"\n",
    "figsize=(9, 10)\n",
    "top_adjust=0.9\n",
    "title_en=False\n",
    "plotCorr(models, dict_df_probs, title, corr_method, figsize=figsize, top_adjust=top_adjust, \n",
    "         title_en=title_en, savefig_path=processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(dict_df_probs['LGBM']['T'].values).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=r'Correlation with $P(D=1|X)$' '\\n' r'when $u(x)=sigmoid(\\beta X)$'\n",
    "# models=['LogisticRegression', 'LGBM']\n",
    "models=['LGBM']\n",
    "# tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T', 'product_T_D_given_T', 'D_pseudo']\n",
    "tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T','D_pseudo']\n",
    "figsize=(7,5)\n",
    "# figsize=(10,7)\n",
    "top_adjust=0.9\n",
    "# tasks=['T', 'D|T']\n",
    "corr_method=stats.pearsonr\n",
    "title_en=True\n",
    "plotCorr_w_Unobs(dict_df_probs, models, title, tasks, alpha=['sigmoid'],X=test_X, corr_method=corr_method,\n",
    "                 figsize=figsize, top_adjust=top_adjust, title_en=title_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b90768",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=r'Correlation with $P(D=1|X)$' '\\n' r'when $u(x)=\\alpha P(D=1|T=1,X)$'\n",
    "# models=['LogisticRegression', 'LGBM']\n",
    "models=['LGBM']\n",
    "# tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T', 'product_T_D_given_T', 'D_pseudo']\n",
    "tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T','D_pseudo']\n",
    "figsize=(7,5)\n",
    "# figsize=(10,7)\n",
    "top_adjust=0.9\n",
    "# tasks=['T', 'D|T']\n",
    "corr_method=stats.pearsonr\n",
    "loc=\"center left\"\n",
    "# loc=\"lower left\"\n",
    "title_en=False\n",
    "plotCorr_w_Unobs(dict_df_probs, models, title, tasks, alpha=np.arange(0,1.1,0.1), corr_method=corr_method,\n",
    "                 figsize=figsize, top_adjust=top_adjust, title_en=title_en, loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=r'Correlation with $P(D=1|X)$' '\\n' r'when $u(x)=\\alpha$ is constant'\n",
    "models=['LGBM']\n",
    "# tasks=['T', 'D|T']\n",
    "corr_method=stats.pearsonr\n",
    "tasks=['T', 'D|T', 'D|T_ipw', 'D_and_T','D_pseudo']\n",
    "figsize=(7,5)\n",
    "# figsize=(10,7)\n",
    "# loc=\"center left\"\n",
    "loc=\"lower left\"\n",
    "title_en=False\n",
    "plotCorr_w_Unobs(dict_df_probs, models, title, tasks, corr_method=corr_method,\n",
    "                 figsize=figsize, loc=loc, title_en=title_en, custome_ticker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs_stats=pd.DataFrame({'model' : [],'task':[],'mean':[],'std':[], 'min':[], 'max':[]})\n",
    "tasks=['T','D|T']\n",
    "for m in models:\n",
    "    for t in tasks:\n",
    "        df_probs_stats=df_probs_stats.append({'model' : m,'task':t,'mean':dict_df_probs[m][t].mean(),\n",
    "                               'std':dict_df_probs[m][t].std(), 'min':dict_df_probs[m][t].min(),\n",
    "                                'max': dict_df_probs[m][t].max()}, ignore_index=True)\n",
    "        print(f\" mean , std, min. for model {m} and task {t}: {dict_df_probs[m][t].mean():.3f}, {dict_df_probs[m][t].std():.3f}, {dict_df_probs[m][t].min():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191445b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f0f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Distribution of estimated probs'\n",
    "models=['LGBM']\n",
    "tasks=['D|T']\n",
    "figsize=(10,7)\n",
    "plotDistributionProbs(dict_df_probs, models, title, tasks, figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e59622",
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Distribution of estimated probs'\n",
    "models=['LGBM']\n",
    "tasks=['T']\n",
    "figsize=(10,7)\n",
    "plotDistributionProbs(dict_df_probs, models, title, tasks, figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    print(f\"for model :{m}\")\n",
    "    getResiduals(dict_df_probs[m]['D_and_T'], dict_df_probs[m]['D|T'], dict_df_probs[m]['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a99acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_labels['LGBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids_test = pd.DataFrame([])\n",
    "df_labels_copy = df_labels.loc[test_idxs].copy()\n",
    "all_pids = df_labels_copy['pids']\n",
    "df_ids_test['T'] = all_pids.copy()\n",
    "df_ids_test['D|T'] = np.nan\n",
    "df_ids_test.loc[~pd.isna(df_labels_copy['D']),['D|T']] = all_pids.loc[~pd.isna(df_labels_copy['D'])]\n",
    "df_ids_test['D_and_T'] = np.nan\n",
    "df_ids_test.loc[~pd.isna(df_labels_copy['D']),['D_and_T']] = all_pids.loc[~pd.isna(df_labels_copy['D'])]\n",
    "print(df_ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e81ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids_test_wo_cutoff = pd.DataFrame([])\n",
    "df_labels_wo_cutoff_copy = df_labels_wo_cutoff.loc[test_idxs_wo_cutoff].copy()\n",
    "all_pids_wo_cutoff = df_labels_wo_cutoff_copy['pids']\n",
    "df_ids_test_wo_cutoff['T'] = all_pids_wo_cutoff.copy()\n",
    "df_ids_test_wo_cutoff['D|T'] = np.nan\n",
    "df_ids_test_wo_cutoff.loc[~pd.isna(df_labels_wo_cutoff_copy['D|T']),['D|T']] = \\\n",
    "                            all_pids_wo_cutoff.loc[~pd.isna(df_labels_wo_cutoff_copy['D|T'])]\n",
    "print(df_ids_test_wo_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22850441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_wo_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c819931",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = []\n",
    "patients_already_seen = set()\n",
    "for i in range(250):\n",
    "    endpoints.append(pd.read_parquet(\n",
    "        '/share/pierson/selective_labels_data/hirid_data_analysis/richras_dir/learning_from_doctor_and_patient/output_directory/endpoints_w_lac/batch_%i.parquet' % i))\n",
    "    assert len(patients_already_seen.intersection(set(endpoints[-1]['patientid']))) == 0 # This assert checks that \n",
    "    # only new patient ids are added, i.e patientid is a unique field\n",
    "    old_len = len(patients_already_seen)\n",
    "    patients_already_seen = patients_already_seen.union(set(endpoints[-1]['patientid']))\n",
    "    # print(len(patients_already_seen) - old_len, 'new patients')\n",
    "endpoints = pd.concat(endpoints)\n",
    "print(endpoints.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363adc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_table = pd.read_parquet('/share/pierson/selective_labels_data/hirid_data_analysis/output_directory/general_table_extended.parquet')\n",
    "mort_status = general_table[['patientid', 'discharge_status']].copy()\n",
    "mortality_dict={\"alive\":0, \"dead\":1}\n",
    "mort_status['status']=mort_status['discharge_status'].map(mortality_dict)\n",
    "length_staySeries = endpoints.groupby(['patientid'])['rel_datetime'].count().reset_index()\n",
    "#first column is patientid, second is relative length of stay\n",
    "length_stay = np.array(length_staySeries.values)\n",
    "print(length_stay, length_stay.shape)\n",
    "mort_status_rel_stay = pd.merge(mort_status, pd.DataFrame(length_staySeries), on='patientid')\n",
    "mort_status_rel_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9eefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long term outcomes\n",
    "\n",
    "def set_axis_style(ax, labels, title, xlabel, ylabel):\n",
    "    ax.xaxis.set_tick_params(direction='out')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1), labels=labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    ax.set_xlabel(xlabel+\" (Binary Label)\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)    \n",
    "\n",
    "def getCommonPids(pid_ls1, pid_ls2):\n",
    "    ret_pids1, ret_pids2=[],[]\n",
    "    intersect_pids=np.intersect1d(pid_ls1, pid_ls2)\n",
    "    for pid in intersect_pids:\n",
    "        ret_pids1.append(np.argwhere(pid_ls1==pid).item())\n",
    "        ret_pids2.append(np.argwhere(pid_ls2==pid).item())\n",
    "    assert len(intersect_pids)==len(ret_pids1)==len(ret_pids2)\n",
    "    assert len(intersect_pids)>1\n",
    "    return intersect_pids, ret_pids1, ret_pids2\n",
    "\n",
    "def getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel, df_metrics, dict_df_ids_cutoff, dict_df_labels_cutoff, dict_df_probs_cutoff, dict_df_labels, dict_df_ids, **kwargs):\n",
    "    \"\"\" Get metrics between task A and B and longer time outcomes\n",
    "    args:\n",
    "        estimators[0][mName]={}, estimators[1][mName]={}\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 9), sharey=True)\n",
    "    i,j=0,0\n",
    "        \n",
    "    outcome_D_name=kwargs.get('outcome_D_name')\n",
    "    ipw=kwargs.get('ipw')\n",
    "    for mName in modelNames:\n",
    "        \n",
    "        idx_untested=np.where(dict_df_labels_cutoff['T']==0)[0]\n",
    "        \n",
    "        T_labels = np.array(dict_df_labels['T'].values.tolist())\n",
    "        T_ids = np.array(dict_df_ids['T'].values.tolist())\n",
    "        \n",
    "        untested_T_cutoff_ids = np.array(dict_df_ids_cutoff['T'].values.tolist())\n",
    "        \n",
    "        T_labels = getLabels(T_labels, T_ids, untested_T_cutoff_ids)\n",
    "        untested_T_labels=T_labels[idx_untested][~np.isnan(T_labels[idx_untested])]\n",
    "        \n",
    "        untested_T_probs = np.array(dict_df_probs_cutoff['T'].values.tolist())[idx_untested]\n",
    "        untested_T_probs=untested_T_probs[~np.isnan(T_labels[idx_untested])]\n",
    "        untested_D_probs = np.array(dict_df_probs_cutoff[str(tasks[1])].values.tolist())[idx_untested]\n",
    "        untested_D_probs=untested_D_probs[~np.isnan(T_labels[idx_untested])] \n",
    "        \n",
    "        assert len(untested_T_labels)==len(untested_T_probs)\n",
    "        print(f\"labels for untested T=0 pop for model {str(mName)}, :{len(untested_T_probs)}\")\n",
    "        print(f\"correlation 1x1:\"\n",
    "        f\"{stats.spearmanr(untested_T_labels, untested_T_probs)}\")\n",
    "        auc_1_1, pr_1_1, ba_1_1=getMetrics(untested_T_labels, untested_T_probs)[:3]\n",
    "        cr_1_1=getMetrics(untested_T_labels, untested_T_probs)[-1]\n",
    "        print(f\"PR, balanced accuracy metrics for 1x1 :{str(mName)}, {pr_1_1},{ba_1_1} \\n {cr_1_1}\")\n",
    "        print(f\"auc score 1x1:{auc_1_1:.4f}\")\n",
    "        \n",
    "        auc_2_1, pr_2_1, ba_2_1=getMetrics(untested_T_labels, untested_D_probs)[:3]\n",
    "        cr_2_1=getMetrics(untested_T_labels, untested_D_probs)[-1]\n",
    "        print(f\"correlation 2x1:{stats.spearmanr(untested_T_labels, untested_D_probs)}\")\n",
    "        print(f\"PR, balanced accuracy metrics for 2x1 :{str(mName)}, {pr_2_1},{ba_2_1} \\n {cr_2_1}\")\n",
    "        print(f\"auc score 2x1:{auc_2_1:.4f}\")\n",
    "        \n",
    "        df_metrics=df_metrics.append({'AUC' :auc_1_1 ,'PR':pr_1_1,'BalancedAcc':ba_1_1, 'modelName':mName,'rowName':estNames[0],'colName':'T'},ignore_index=True)\n",
    "        df_metrics=df_metrics.append({'AUC' :auc_2_1 ,'PR':pr_2_1,'BalancedAcc':ba_2_1,'modelName':mName,'rowName':estNames[1],'colName':'T'},ignore_index=True)\n",
    "\n",
    "        ax[i,j].violinplot([untested_T_probs[untested_T_labels==i] for i in [0,1]], \n",
    "                           showmeans=False, showmedians=True,showextrema=True)\n",
    "        ax[i+1,j].violinplot([untested_D_probs[untested_T_labels==i] for i in [0,1]],\n",
    "                             showmeans=False, showmedians=True,showextrema=True)\n",
    "        \n",
    "        #2,2 and 1,2\n",
    "        idx_untested_D_given_T = np.where((dict_df_labels_cutoff['T']==0) & \n",
    "                                            (T_labels==1)[0])\n",
    "                \n",
    "        T_ids = np.array(dict_df_ids['T'].values.tolist())\n",
    "#         D_given_T_labels = np.array(dict_df_labels[str(tasks[1])].values.tolist())\n",
    "#         D_given_T_labels = getLabels(D_given_T_labels, T_ids, untested_T_cutoff_ids)\n",
    "#         untested_D_given_T_labels = D_given_T_labels[idx_untested_D_given_T][~np.isnan(D_given_T_labels[idx_untested_D_given_T])]\n",
    "        \n",
    "        outcome_D_labels = np.array(dict_df_labels[outcome_D].values.tolist())\n",
    "        outcome_D_labels = getLabels(outcome_D_labels, T_ids, untested_T_cutoff_ids)\n",
    "        untested_outcome_D_labels=outcome_D_labels[idx_untested_D_given_T][~np.isnan(outcome_D_labels[idx_untested_D_given_T])]\n",
    "        \n",
    "        untested_T_probs = np.array(dict_df_probs_cutoff['T'].values.tolist())[idx_untested_D_given_T]\n",
    "        untested_T_probs=untested_T_probs[~np.isnan(outcome_D_labels[idx_untested_D_given_T])]\n",
    "        untested_D_probs = np.array(dict_df_probs_cutoff[str(tasks[1])].values.tolist())[idx_untested_D_given_T]\n",
    "        untested_D_probs=untested_D_probs[~np.isnan(outcome_D_labels[idx_untested_D_given_T])]\n",
    "        \n",
    "#         print(f\"labels for untested T=0 after 1 hour and D=1 pop for model {str(mName)}, :{len(untested_D_given_T_labels)}\")\n",
    "        print(f\"correlation 1x2:\"\n",
    "        f\"{stats.spearmanr(untested_outcome_D_labels, untested_T_probs)}\")\n",
    "        \n",
    "        print(f\"correlation 2x2:\"\n",
    "        f\"{stats.spearmanr(untested_outcome_D_labels, untested_D_probs)}\")\n",
    "        auc_1_2, pr_1_2, ba_1_2=getMetrics(untested_outcome_D_labels, untested_T_probs)[:3]\n",
    "        cr_1_2=getMetrics(untested_outcome_D_labels, untested_T_probs)[-1]\n",
    "        print(f\"auc score 1x2:{auc_1_2:.4f}\")\n",
    "        \n",
    "        auc_2_2, pr_2_2, ba_2_2=getMetrics(untested_outcome_D_labels, untested_D_probs)[:3]\n",
    "        cr_2_2=getMetrics(untested_outcome_D_labels, untested_D_probs)[-1]\n",
    "        print(f\"auc score 2x2:{auc_2_2:.4f}\")\n",
    "        print(f\"PR, balanced accuracy metrics for 1x2 :{str(mName)}, {pr_1_2},{ba_1_2} \\n {cr_1_2}\")\n",
    "\n",
    "        print(f\"PR, balanced accuracy metrics for 2x2 :{str(mName)}, {pr_2_2},{ba_2_2} \\n {cr_2_2}\")\n",
    "\n",
    "        df_metrics=df_metrics.append({'AUC' :auc_1_2 ,'PR':pr_1_2,'BalancedAcc':ba_1_2,'modelName':mName,'rowName':estNames[0],'colName':f\"{outcome_D_name}\"},ignore_index=True)\n",
    "        df_metrics=df_metrics.append({'AUC' :auc_2_2 ,'PR':pr_2_2,'BalancedAcc':ba_2_2,'modelName':mName,'rowName':estNames[1],'colName':f\"{outcome_D_name}\"},ignore_index=True)\n",
    "\n",
    "        ax[i,j+1].violinplot([untested_T_probs[untested_outcome_D_labels==i] for i in [0,1]], \n",
    "                             showmeans=False, showmedians=True, showextrema=True)\n",
    "        ax[i+1,j+1].violinplot([untested_D_probs[untested_outcome_D_labels==i] for i in [0,1]],\n",
    "                               showmeans=False, showmedians=True, showextrema=True)\n",
    "\n",
    "        #1,3 and 2,3\n",
    "\n",
    "        common_mort_pids, idx_labels_mort, idx_probs_mort =\\\n",
    "                getCommonPids(mort_status_rel_stay.loc[(~pd.isnull(mort_status_rel_stay['discharge_status']))\\\n",
    "                            & (mort_status_rel_stay['rel_datetime']>12)]['patientid'].values.tolist(),\n",
    "                untested_T_cutoff_ids[idx_untested])\n",
    "        mort_labels=np.array(mort_status_rel_stay.loc[(~pd.isnull(mort_status_rel_stay['discharge_status']))\\\n",
    "                            & (mort_status_rel_stay['rel_datetime']>12)]['status'].values.tolist())[idx_labels_mort]\n",
    "\n",
    "        assert np.isnan(mort_labels).sum()==0\n",
    "        \n",
    "        untested_T_probs = np.array(dict_df_probs_cutoff['T'].values.tolist())[idx_untested]\n",
    "        untested_D_probs = np.array(dict_df_probs_cutoff[str(tasks[1])].values.tolist())[idx_untested]\n",
    "\n",
    "        print(f\"labels for untested T=0 pop for model {str(mName)} and mort status, :{len(common_mort_pids)}\")\n",
    "\n",
    "        print(f\"correlation 1x3:\"\n",
    "        f\"{stats.spearmanr(mort_labels, untested_T_probs[idx_probs_mort])}\")\n",
    "        auc_1_3, pr_1_3, ba_1_3=getMetrics(mort_labels, untested_T_probs[idx_probs_mort])[:3]\n",
    "        cr_1_3=getMetrics(mort_labels, untested_T_probs[idx_probs_mort])[-1]\n",
    "        print(f\"PR, balanced accuracy metrics for 1x3 :{str(mName)}, {pr_1_3},{ba_1_3} \\n {cr_1_3}\")\n",
    "        print(f\"correlation 2x3:\"\n",
    "        f\"{stats.spearmanr(mort_labels, untested_D_probs[idx_probs_mort])}\")\n",
    "        auc_2_3, pr_2_3, ba_2_3=getMetrics(mort_labels, untested_D_probs[idx_probs_mort])[:3]\n",
    "        cr_2_3=getMetrics(mort_labels, untested_D_probs[idx_probs_mort])[-1]\n",
    "        print(f\"PR, balanced accuracy metrics for 2x3 :{str(mName)}, {pr_2_3},{ba_2_3} \\n {cr_2_3}\")\n",
    "        print(f\"auc score 1x3:{auc_1_3:.4f}\")\n",
    "        print(f\"auc score 2x3:{auc_2_3:.4f}\")\n",
    "        df_metrics=df_metrics.append({'AUC' :auc_1_3 ,'PR':pr_1_3,'BalancedAcc':ba_1_3,'modelName':mName,'rowName':estNames[0],'colName':'Mortality'},ignore_index=True)\n",
    "        df_metrics=df_metrics.append({'AUC' :auc_2_3 ,'PR':pr_2_3,'BalancedAcc':ba_2_3,'modelName':mName,'rowName':estNames[1],'colName':'Mortality'},ignore_index=True)\n",
    "\n",
    "        ax[i,j+2].violinplot([untested_T_probs[idx_probs_mort][mort_labels==i] for i in [0,1]], \n",
    "                             showmeans=False, showmedians=True, showextrema=True)\n",
    "        ax[i+1,j+2].violinplot([untested_D_probs[idx_probs_mort][mort_labels==i] for i in [0,1]],\n",
    "                               showmeans=False, showmedians=True, showextrema=True)\n",
    "        print(f\"len of alive:{len([untested_T_probs[idx_probs_mort][mort_labels==i] for i in [0]][0])}\")\n",
    "        print(f\"len of dead:{len([untested_T_probs[idx_probs_mort][mort_labels==i] for i in [1]][0])}\")\n",
    "        # set style for the axes\n",
    "        labels = ['0', '1']\n",
    "        \n",
    "        for k,a in enumerate([ax[0,0], ax[0,1], ax[0,2], ax[1,0], ax[1,1], ax[1,2]]):\n",
    "            set_axis_style(a, labels, title[k], xLabel[k], yLabel[k])\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55714d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_probs['LGBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_wo_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics=pd.DataFrame({'AUC' : [],'PR':[],'BalancedAcc':[],'modelName':[],'rowName':[],'colName':[]})\n",
    "modelNames=['LGBM']\n",
    "tasks=['T', 'D|T']\n",
    "estNames=[r'$p(T_{t=1hr}=1|X)$', r'$p(D_{t=1hr}=1|T_{t=1hr}=1,X)$']\n",
    "title=[r'$p(T_{t=1 hr}) \\quad X \\quad T_{ever}$', r'$p(T_{t=1 hr}) \\quad X \\quad D|T_{ever}$',\\\n",
    "               r'$p(T_{t=1 hr}) \\quad X \\quad MortStatus$', r'$p(D_{t=1 hr}|T_{t=1 hr}) \\quad X \\quad T_{ever}$',\\\n",
    "               r'$p(D_{t=1 hr}|T_{t=1 hr}) \\quad X \\quad D|T_{ever}$', r'$p(D_{t=1 hr}|T_{t=1 hr}) \\quad X \\quad MortStatus$']\n",
    "yLabel=[r'$p(T_{t=1 hr})$', r'$p(T_{t=1 hr})$',\\\n",
    "       r'$p(T_{t=1 hr})$', r'$p(D_{t=1 hr}|T_{t=1 hr})$',\\\n",
    "       r'$p(D_{t=1 hr}|T_{t=1 hr}) $', r'$p(D_{t=1 hr}|T_{t=1 hr})$']\n",
    "xLabel=[r'$T_{ever}$', r'$D|T_{ever}$',\\\n",
    "       r'$MortStatus$', r'$T_{ever}$',\\\n",
    "       r'$D|T_{ever}$', r'$MortStatus$']\n",
    "outcome_D='D|T'\n",
    "outcome_D_name='D|T'\n",
    "df_metrics=getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel,\n",
    "df_metrics, df_ids_test, dict_df_labels['LGBM'], dict_df_probs['LGBM'], df_labels_wo_cutoff, \n",
    "                                    df_ids_test_wo_cutoff, outcome_D_name=outcome_D_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c67daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames=['LGBM']\n",
    "tasks=['T', 'D_and_T']\n",
    "estNames=[r'$p(T_{t=1hr}=1|X)$', r'$p(D_{t=1hr}=1,T_{t=1hr}=1|X)$']\n",
    "title=[r'$p(T_{t=1 hr}) \\quad X \\quad T_{ever}$', r'$p(T_{t=1 hr}) \\quad X \\quad D,T_{ever}$',\\\n",
    "               r'$p(T_{t=1 hr}) \\quad X \\quad MortStatus$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad T_{ever}$',\\\n",
    "               r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad D|T_{ever}$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad MortStatus$']\n",
    "yLabel=[r'$p(T_{t=1 hr})$', r'$p(T_{t=1 hr})$',\\\n",
    "       r'$p(T_{t=1 hr})$', r'$p(D_{t=1 hr},T_{t=1 hr})$',\\\n",
    "       r'$p(D_{t=1 hr},T_{t=1 hr}) $', r'$p(D_{t=1 hr},T_{t=1 hr})$']\n",
    "xLabel=[r'$T_{ever}$', r'$D,T_{ever}$',\\\n",
    "       r'$MortStatus$', r'$T_{ever}$',\\\n",
    "       r'$D,T_{ever}$', r'$MortStatus$']\n",
    "outcome_D='D_and_T'\n",
    "outcome_D_name='D,T'\n",
    "df_metrics=getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel,\n",
    "df_metrics, df_ids_test, dict_df_labels['LGBM'], dict_df_probs['LGBM'], df_labels_wo_cutoff, \n",
    "                                    df_ids_test_wo_cutoff, outcome_D_name=outcome_D_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames=['LGBM']\n",
    "tasks=['T', 'D_and_T']\n",
    "estNames=[r'$p(T_{t=1hr}=1|X)$', r'$p(D_{t=1hr}=1,T_{t=1hr}=1|X)$']\n",
    "outcome_D='D|T'\n",
    "outcome_D_name='D|T'\n",
    "title=[r'$p(T_{t=1 hr}) \\quad X \\quad T_{ever}$', r'$p(T_{t=1 hr}) \\quad X \\quad D|T_{ever}$',\\\n",
    "               r'$p(T_{t=1 hr}) \\quad X \\quad MortStatus$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad T_{ever}$',\\\n",
    "               r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad D|T_{ever}$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad MortStatus$']\n",
    "yLabel=[r'$p(T_{t=1 hr})$', r'$p(T_{t=1 hr})$',\\\n",
    "       r'$p(T_{t=1 hr})$', r'$p(D_{t=1 hr},T_{t=1 hr})$',\\\n",
    "       r'$p(D_{t=1 hr},T_{t=1 hr}) $', r'$p(D_{t=1 hr},T_{t=1 hr})$']\n",
    "xLabel=[r'$T_{ever}$', r'$D|T_{ever}$',\\\n",
    "       r'$MortStatus$', r'$T_{ever}$',\\\n",
    "       r'$D|T_{ever}$', r'$MortStatus$']\n",
    "df_metrics=getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel,\n",
    "df_metrics, df_ids_test, dict_df_labels['LGBM'], dict_df_probs['LGBM'], df_labels_wo_cutoff, \n",
    "                                    df_ids_test_wo_cutoff, outcome_D_name=outcome_D_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bff525",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames=['LGBM']\n",
    "tasks=['T', 'D|T_ipw']\n",
    "estNames=[r'$p(T_{t=1hr}=1|X)$', r'$IPW_{t=1hr}$']\n",
    "outcome_D='D|T'\n",
    "outcome_D_name='D|T'\n",
    "title=[r'$p(T_{t=1 hr}) \\quad X \\quad T_{ever}$', r'$p(T_{t=1 hr}) \\quad X \\quad D|T_{ever}$',\\\n",
    "               r'$p(T_{t=1 hr}) \\quad X \\quad MortStatus$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad T_{ever}$',\\\n",
    "               r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad D|T_{ever}$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad MortStatus$']\n",
    "yLabel=[r'$p(T_{t=1 hr})$', r'$p(T_{t=1 hr})$',\\\n",
    "       r'$p(T_{t=1 hr})$', r'$p(D_{t=1 hr},T_{t=1 hr})$',\\\n",
    "       r'$p(D_{t=1 hr},T_{t=1 hr}) $', r'$p(D_{t=1 hr},T_{t=1 hr})$']\n",
    "xLabel=[r'$T_{ever}$', r'$D|T_{ever}$',\\\n",
    "       r'$MortStatus$', r'$T_{ever}$',\\\n",
    "       r'$D|T_{ever}$', r'$MortStatus$']\n",
    "df_metrics=getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel,\n",
    "df_metrics, df_ids_test, dict_df_labels['LGBM'], dict_df_probs['LGBM'], df_labels_wo_cutoff, \n",
    "                                    df_ids_test_wo_cutoff, outcome_D_name=outcome_D_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0caa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames=['LGBM']\n",
    "tasks=['T', 'D_pseudo']\n",
    "estNames=[r'$p(T_{t=1hr}=1|X)$', r'Pseudo Labels']\n",
    "outcome_D='D|T'\n",
    "outcome_D_name='D|T'\n",
    "title=[r'$p(T_{t=1 hr}) \\quad X \\quad T_{ever}$', r'$p(T_{t=1 hr}) \\quad X \\quad D|T_{ever}$',\\\n",
    "               r'$p(T_{t=1 hr}) \\quad X \\quad MortStatus$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad T_{ever}$',\\\n",
    "               r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad D|T_{ever}$', r'$p(D_{t=1 hr},T_{t=1 hr}) \\quad X \\quad MortStatus$']\n",
    "yLabel=[r'$p(T_{t=1 hr})$', r'$p(T_{t=1 hr})$',\\\n",
    "       r'$p(T_{t=1 hr})$', r'$p(D_{t=1 hr},T_{t=1 hr})$',\\\n",
    "       r'$p(D_{t=1 hr},T_{t=1 hr}) $', r'$p(D_{t=1 hr},T_{t=1 hr})$']\n",
    "xLabel=[r'$T_{ever}$', r'$D|T_{ever}$',\\\n",
    "       r'$MortStatus$', r'$T_{ever}$',\\\n",
    "       r'$D|T_{ever}$', r'$MortStatus$']\n",
    "df_metrics=getLongerOutcomesMetrics(modelNames, tasks, estNames, outcome_D, title, xLabel, yLabel,\n",
    "df_metrics, df_ids_test, dict_df_labels['LGBM'], dict_df_probs['LGBM'], df_labels_wo_cutoff, \n",
    "                                    df_ids_test_wo_cutoff, outcome_D_name=outcome_D_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc893db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLongerOutcomes(df_metrics, plot_title, subsetBy, subfig, selectrowNames, figsize=(15,7)):\n",
    "    tab_colors = ['tab:blue', 'tab:orange']\n",
    "    ax1=subfig.add_subplot(1,2,1)\n",
    "    df_temp = df_metrics[df_metrics['colName']==subsetBy][['AUC','modelName','rowName']]\n",
    "    df_temp.index = df_temp.rowName\n",
    "    df_temp = df_temp.drop_duplicates()\n",
    "    sns.barplot(data=df_temp, x='rowName', y='AUC', hue='rowName', ax=ax1, dodge=False, palette=tab_colors)\n",
    "    ax1.get_legend().remove()\n",
    "#     ax1.set_xticklabels(selectrowNames, rotation=90,fontsize=20)\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.set_xlabel('')\n",
    "    ylabel = ax1.get_ylabel()\n",
    "    ax1.set_ylabel(ylabel, fontsize=20)\n",
    "    ax1.tick_params(axis='y', labelsize=15)\n",
    "    ax1.yaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%.2f'))\n",
    "\n",
    "\n",
    "    ax2=subfig.add_subplot(1,2,2)\n",
    "    df_temp = df_metrics[df_metrics['colName']==subsetBy][['PR','modelName','rowName']]\n",
    "    df_temp.index = df_temp.rowName\n",
    "    df_temp = df_temp.drop_duplicates()\n",
    "    sns.barplot(data=df_temp, x='rowName', y='PR', hue='rowName', ax=ax2, dodge=False,palette=tab_colors)\n",
    "    ax2.get_legend().remove()\n",
    "#     ax2.set_xticklabels(selectrowNames, rotation=90,fontsize=20)\n",
    "    ax2.set_xticklabels([])\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.set_xlabel('')\n",
    "    ylabel = ax2.get_ylabel()\n",
    "    ax2.yaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%.2f'))\n",
    "    ax2.set_ylabel(ylabel, fontsize=20)\n",
    "    ax2.tick_params(axis='y', labelsize=15)\n",
    "    \n",
    "#     fig.legend(loc='center right',bbox_to_anchor=(1, 1.3), fontsize=15)\n",
    "#     subfig.tight_layout()\n",
    "    subfig.suptitle(plot_title, fontsize=20)\n",
    "    subfig.subplots_adjust(top=0.9, wspace = 0.5)\n",
    "    return subfig, ax1.get_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f991cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,5)\n",
    "fig=plt.figure(figsize=figsize)\n",
    "subfig=fig.subfigures(nrows=1, ncols=3)\n",
    "\n",
    "# Subplot 1\n",
    "plot_title=r'$predicting \\quad T_{t>1hr}$'\n",
    "subsetBy='T'\n",
    "selectModels=['LGBM']\n",
    "figsize=(7,4)\n",
    "df_metrics_T=df_metrics.copy()\n",
    "df_metrics_T=df_metrics_T.loc[df_metrics_T['modelName'].isin(selectModels)]\n",
    "selectrowNames=[r'$p(T_{t=1hr}=1|X)$', r'$p(D_{t=1hr}=1|T_{t=1hr}=1,X)$', r'$p(D_{t=1hr}=1,T_{t=1hr}=1|X)$', r'$IPW_{t=1hr}$', r'Pseudo Labels']\n",
    "selectcolNames=['T', 'D|T', 'Mortality']\n",
    "# df_metrics_T['colName'] = df_metrics_T['colName'].apply(lambda x: x in selectTasks)\n",
    "df_metrics_T=df_metrics_T.loc[df_metrics_T['rowName'].isin(selectrowNames)]\n",
    "df_metrics_T=df_metrics_T.loc[df_metrics_T['colName'].isin(selectcolNames)]\n",
    "\n",
    "subfig[0], _ =plotLongerOutcomes(df_metrics_T, plot_title, subsetBy, subfig[0], selectrowNames, figsize=figsize)\n",
    "\n",
    "# Subplot 2\n",
    "plot_title=r'$predicting \\quad D|T_{t>1hr}$'\n",
    "subsetBy='D|T'\n",
    "selectModels=['LGBM']\n",
    "figsize=(7,4)\n",
    "subfig[1], _ =plotLongerOutcomes(df_metrics_T, plot_title, subsetBy, subfig[1], selectrowNames, figsize=figsize)\n",
    "\n",
    "# Subplot 3\n",
    "plot_title=r'$predicting \\quad Mortality$'\n",
    "subsetBy='Mortality'\n",
    "selectModels=['LGBM']\n",
    "figsize=(7,4)\n",
    "subfig[2], lgnd =plotLongerOutcomes(df_metrics_T, plot_title, subsetBy, subfig[2], selectrowNames, figsize=figsize)\n",
    "plt.legend(handles=lgnd, loc='center', ncol=2, bbox_to_anchor=(-3.6, -0.2), fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_T=df_metrics_T.drop(columns=['BalancedAcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5914f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_T.drop_duplicates(inplace=True)\n",
    "mName='LGBM_w_feat_v2_cutoff_T'\n",
    "path='_depth_7_subsample-data_1.0_subsample-feat_1.0'\n",
    "df_metrics_T_path=osp.join(os.environ.get('USER_PATH'),'HIRID_Repo', 'logs', 'benchmark_exp', str(mName), str(path))\n",
    "df_metrics_T.to_csv(df_metrics_T_path+\"/df_metrics_longTerm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d9d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu-benchmark",
   "language": "python",
   "name": "icu-benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
